"""
Main code for running the GCN model for predicting the best simulator for a given quantum circuit.

Code Blocks
-----------
The main function is broken down into the following code blocks:
    1. Parameters:              Default parameters for the model.
    2. Seed:                    Setting the seed for reproducibility.
    3. Read Data:               Reading the data from the csv file and loading the graphs from the numpy files.
    4. Model Definition:        Defining the model and the optimizer.
    5. Function Definitions:    Functions for training and testing the model.
    6. Training:                Training the model and saving the best model based on validation accuracy.
    7. Verbose Testing:         Testing the model on the test set and printing the predicted simulators for each circuit.

Notes
-----
  Training data:
    * This code written such that the training and testing data is from a single csv file.
    * The csv file should contain the following columns:
        * qasm: The path to the qasm file.
        * SV-8CPU, SV-1CPU, SV-GPU, MPS-8CPU, MPS-1CPU, TN, QDD: Runtimes of the simulators.
        Note: The order of the simulators does not matter, they will be rearranged internally.
        * Simulators can be added or removed by modifying the SIM_LIST parameter.
    * This code looks for the numpy files generated by qasm_read.py.
    * The numpy files generated by qasm_read.py can be manually moved to a different directory if combination of different datasets is required.
    * All the .npy files must be in the same directory and the directory name should be passed as the DATADIR parameter.
    * If different datasets are to be combined, either a new csv file should be created or the `load_graphs` function and the `Read runtimes` section should be modified accordingly.
    * The load graphs function randomly splits the data into train, validation, and test sets based on the split ratio.
    * If specific circuits are to be used for training, validation, or testing, the `load_graphs` function should be modified accordingly.
      - One simple way is to have three different filelists for train, val, and test and pass them to the `load_graphs` function.
      - The load_graphs function should be modified to accept the filelists as arguments and have three separate loops for train, val, and test.
      - The `Read numpy files` section of the loop should remain unchanged.

  Model:
    * The model used comprises of simple graph convolutional layers followed by global sum pooling of the nodes and a fully connected classification layer.
    * The number of outputs are inferred from the `runtimes` variable, i.e., number of elements in `SIM_LIST`.
    * A different model can be used by modifying `qgnn/model/model.py`.

  Training Loss:
    * Training loss is the cross-entropy between the predicted values and the ground truth.
    * The `runtimes_transform` function converts the runtimes to probability distributions and is the most important function that determines the performance of the network.
    * This function can be modified to observe the impact on accuracy.
    * The output of the function must be a softmax to ensure cross-entropy works.
    * However, a different loss can also be used to remove this constraint.
      - Please modify the loss in `train` function definition to achieve this.

  Testing loss and accuracy:
    * The model's prediction is correct if the predicted simulator is within `TEST_THRESHOLD` of the ground-truth fastest simulator.
    * The TEST_THRESHOLD parameter can be varied to observe the effect on accuracy.
    * Testing loss is non-zero when the predicted simulator is beyond TEST_THRESHOLD of the fastest.
    * The printed testing loss is the mean of ratio of (runtime of predicted simulator) to (runtime_of_best_simulator) if (runtime of predicted simulator) > (1 + TEST_THRESHOLD) * (runtime of best simulator).
    * Worst case loss is the largest ratio of (runtime of predicted simulator) to (runtime of best simulator) in the test set.
    * The worst case circuit is the circuit with the worst case loss.

  Training and Testing flags:
    * The model is trained if the TRAIN flag is set to True.
      - The model is saved after each epoch if the validation accuracy improves.
      - The model is also tested on the test set if the validation accuracy improves.
      - The reported test accuracy at the end is with the model with best validation accuracy.
    * The model is tested if the TEST flag is set to True.
      - The model is loaded from the saved model file.
      - The reported test accuracy is with the loaded model.
      - An error is raised if the model file is not found.
    * The model is tested on the test set and the results are printed if VERSBOSE_TEST is set to True.
      - The model is loaded from the saved model file.
      - The following information is printed for each circuit in the test set:
        * Circuit name
        * Actual runtimes of the simulators (in seconds) along with the transformed runtimes used for training
        * Predicted simulators in the order of decreasing probability along with the probabilities.
      - The model can be tested on a single circuit by modifying the `test_data` list before the `Verbose Testing` section.

  Parameters:
    * The parameters can be modified by passing them as keyword arguments to the main function.
    * The default values are listed below the import section of the code.
    * The parameters can also be modified by changing the default values in the code.

  Seed:
    * The seed is set for reproducibility.
    * The following things would change by changing the seed:
      - The random split of the data into train, val, and test sets.
      - The random shuffling of the data in the train dataloader.
      - The random initialization of the model weights.
      - The random dropout of the nodes inside the network.
    * The data and network randomness can be separated by setting different seeds for the data and the network.
      - This can be achieved by adding another `Seed` block of code after the `Read Data` block.
"""

import os
import sys

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader

from qgnn.model.model import GCNet

DATAFILE = 'data/data.csv'  # CSV file containing the list of qasm files along with runtime data.
DATADIR = 'data/mqt/numpy'  # Directory containing the numpy files for the graphs.
SIM_LIST = ['SV-8CPU', 'SV-1CPU', 'SV-GPU', 'MPS-8CPU', 'MPS-1CPU', 'TN', 'QDD']    # List of simulators.

TRAIN_VAL_TEST_SPLIT = (0.6, 0.2, 0.2)  # Split ratio for train, val, and test.
NODE_FEATURES = 17  # Number of node features in the graph. Must be the same as `NODE_EMBEDDING_DIM` in qasm_parser.py.

TRAIN = True    # Flag to train the model.
TEST = False    # Flag to test the model.

GPU_ID = 0  # GPU ID to use for training and testing.

N_LAYERS = 1    # Number of graph convolutional layers.
HIDDEN_CHANNELS = 512   # Number of hidden channels in the graph convolutional layers.
SEED = 2    # Random seed for reproducibility.

EPOCHS = 500    # Number of epochs to train the model.

TEST_THRESHOLD = 0.2  # Threshold of the relative error for the predicted simulator to be considered correct.

VERBOSE_TEST = False    # Flag to print the predicted simulators for each circuit in the test set.


def runtimes_transform(runtimes):
    """
    Transform the runtimes to a probability distribution.

    This function is the most important part of the model.
    Changing this function can change the model's behavior significantly.
    Yet to find the best transformation.

    The output of the function must be a softmax output.
    """
    runtimes = runtimes / runtimes.min(dim=1).values.unsqueeze(1)
    runtimes = F.softmax(-runtimes*5, dim=1)
    return runtimes

def load_graphs(filelist, datadir, runtimes, train_val_test_split):
    """
    Load the graphs from the numpy files.

    Parameters
    ----------
    filelist : list
        List of filenames to load.

    datadir : str
        Directory containing the numpy files.

    runtimes : torch.Tensor
        Tensor containing the runtimes.

    train_val_test_split : tuple
        Tuple containing the split ratio for train, val, and test.

    Returns
    -------
    train_data : list
        List of Data objects for training.

    val_data : list
        List of Data objects for validation.

    test_data : list
        List of Data objects for testing.

    test_circuits : list
        List of circuit names in the test set.
    """
    train_data = []
    val_data = []
    test_data = []
    for i, file in enumerate(filelist):
        ####### Read the numpy files #######
        nodes = np.load(os.path.join(datadir, f'{file}_nodes.npy'))
        edges = np.load(os.path.join(datadir, f'{file}_edges.npy'))

        nodes = torch.tensor(nodes, dtype=torch.float)
        edges = torch.tensor(edges, dtype=torch.long)

        data = Data(
            x=nodes,
            edge_index=edges.t().contiguous(),
            y=runtimes[i].unsqueeze(0),
            circuit_name=file)
        ####################################

        random_number = np.random.rand()
        if random_number < train_val_test_split[0]:
            train_data.append(data)
        elif random_number < train_val_test_split[0] + train_val_test_split[1]:
            val_data.append(data)
        else:
            test_data.append(data)

    return train_data, val_data, test_data

def main(**kwargs):
    ############## Parameters ##############
    datafile = kwargs.get('datafile', DATAFILE)
    datadir = kwargs.get('datadir', DATADIR)
    sim_list = kwargs.get('sim_list', SIM_LIST)
    train_val_test_split = kwargs.get('train_val_test_split', TRAIN_VAL_TEST_SPLIT)
    node_features = kwargs.get('node_features', NODE_FEATURES)
    train_flag = kwargs.get('train', TRAIN)
    test_flag = kwargs.get('test', TEST)
    gpu_id = kwargs.get('gpu_id', GPU_ID)
    n_layers = kwargs.get('n_layers', N_LAYERS)
    hidden_channels = kwargs.get('hidden_channels', HIDDEN_CHANNELS)
    seed = kwargs.get('seed', SEED)
    epochs = kwargs.get('epochs', EPOCHS)
    batch_size = kwargs.get('batch_size', None)
    if batch_size is None:
        batch_size = 2 * (1024 // hidden_channels)**2
        batch_size = min(batch_size, 64)
    test_threshold = kwargs.get('test_threshold', TEST_THRESHOLD)
    verbose_test = kwargs.get('verbose_test', VERBOSE_TEST)
    #######################################

    ############# Seed ##############
    np.random.seed(seed)
    torch.manual_seed(seed)
    #################################

    ########### Read Data ###########
    data = pd.read_csv(datafile)
    # Necessary headers: qasm, SV-8CPU, SV-1CPU, SV-GPU, MPS-8CPU, MPS-1CPU, TN, QDD
    filelist = data['qasm'].tolist()
    filelist = [file[1:] for file in filelist]
    filelist = [os.path.splitext(file)[0] for file in filelist]

    ### Read runtimes
    runtimes = data[sim_list]
    # Convert to torch tensor
    # Convert 'error' to 1200
    runtimes = runtimes.applymap(lambda x: 1200 if x == 'error' else float(x))
    runtimes = runtimes.to_numpy(na_value=1200)
    runtimes = torch.tensor(runtimes, dtype=torch.float)
    runtimes_actual = runtimes.clone()

    runtimes = runtimes_transform(runtimes)
    runtimes = torch.stack([runtimes, runtimes_actual], dim=1)
    ###

    train_data, val_data, test_data = load_graphs(filelist, datadir, runtimes, train_val_test_split)

    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)
    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)
    #################################


    device = torch.device(f'cuda:{gpu_id}' if torch.cuda.is_available() else 'cpu')


    ########### Model Definition ############
    model = GCNet(
        num_node_features=node_features,
        n_layers=n_layers,
        hidden_channels=hidden_channels,
        num_classes=runtimes.shape[-1]
    ).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
    lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.98)
    run_name = f"gcnconv_{n_layers}_{hidden_channels}_{seed}_{test_threshold}_{epochs}"
    print(f'Run Name: {run_name}')
    #########################################

    ########### Function Definitions ###########
    def train(epoch):
        model.train()

        loss_all = 0
        for data in train_loader:
            data = data.to(device)
            optimizer.zero_grad()
            output = model(data)

            # Cross-entropy loss
            loss = -torch.sum(output * data.y[...,0,:], dim=1).mean()

            loss.backward()
            loss_all += loss.item() * output.size(0)
            optimizer.step()
        return loss_all / len(train_loader.dataset)

    def test(loader):
        model.eval()

        correct = 0
        pred_loss = 0
        worst_case = 0
        worst_case_circuit = ''
        with torch.no_grad():
            for data in loader:
                data = data.to(device)
                output = model(data)
                pred = output.max(1)[1]
                gt_runtimes = data.y[...,1,:]
                gt_runtimes = (gt_runtimes / torch.min(gt_runtimes, dim=1).values.unsqueeze(1))

                # Select the values of gt_runtimes indexed by pred
                predicted_simulator_loss = gt_runtimes[torch.arange(gt_runtimes.size(0)), pred.squeeze()]
                correct += (predicted_simulator_loss < (test_threshold + 1)).sum().item()

                # Add to loss if the predicted simulator is worse than threshold
                predicted_simulator_loss[predicted_simulator_loss <= (test_threshold + 1)] = 0
                pred_loss += predicted_simulator_loss.sum().item()
                if worst_case < predicted_simulator_loss.max():
                    worst_case = predicted_simulator_loss.max().item()
                    worst_case_idx = predicted_simulator_loss.argmax().item()
                    worst_case_circuit = data.circuit_name[worst_case_idx]


        return correct / len(loader.dataset), pred_loss / (len(loader.dataset) - correct), worst_case, worst_case_circuit
    ###########################################

    ########### Training ############
    os.makedirs('saved_models', exist_ok=True)
    test_loss = test_worst = float('inf')
    best_val_acc = test_acc = 0
    if train_flag:
        for epoch in range(1, epochs+1):
            train_loss = train(epoch)
            lr_scheduler.step()
            val_acc, val_loss, _, _ = test(val_loader)
            if best_val_acc < val_acc:
                best_val_acc = val_acc
                test_acc, test_loss, test_worst, worst_circuit = test(test_loader)
                torch.save(model.state_dict(), f'saved_models/{run_name}.pt')
            print_str = f'Epoch: {epoch}, Train Loss: {train_loss:.3f}, Val Acc: {val_acc:.3f}, Val Loss: {val_loss:.3f}, Test Acc: {test_acc:.3f}, Test Loss: {test_loss:.3f}'
            sys.stdout.write(f"\r\033[K{print_str}")
            sys.stdout.flush()
        print_str = f'Epoch: {epoch}, Train Loss: {train_loss:.3f}, Val Acc: {val_acc:.3f}, Val Loss: {val_loss:.3f}, Test Acc: {test_acc:.3f}, Test Loss: {test_loss:.3f}, Worst Case: {test_worst:.3f}, Worst Circuit: {worst_circuit}'
        sys.stdout.write(f"\r\033[K{print_str}\n")
        sys.stdout.flush()

    if test_flag:
        model.load_state_dict(torch.load(f'saved_models/{run_name}.pt', map_location=device))
        test_acc, test_loss, test_worst, worst_circuit = test(test_loader)

    if train_flag or test_flag:
        print(f'Best Test Accuracy: {test_acc:.3f}, Test Loss: {test_loss:.3f}, Worst Case: {test_worst:.3f}, Worst Circuit: {worst_circuit}')
    ################################

    ########### Verbose Testing ############
    if verbose_test:
        # Verbose testing
        model.load_state_dict(torch.load(f'saved_models/{run_name}.pt', map_location=device))
        model.eval()
        with torch.no_grad():
            for i, data in enumerate(test_data):
                data = data.to(device)
                output = model(data)
                output = torch.exp(output)
                print(f'Circuit: {data.circuit_name}:')
                print(f'  Actual Runtime:')
                for j, sim in enumerate(sim_list):
                    print(f'    {sim+":":>10} {data.y[...,1,j].squeeze().item():<10.4f} {data.y[...,0,j].squeeze().item():.4f}')
                print(f'  Predicted Simulators:')
                sim_order = output.argsort(descending=True).squeeze().tolist()
                for j in sim_order:
                    print(f'    {sim_list[j]+":":>10} {output[0][j]:.4f}')
                print()
    #########################################

    return test_acc, test_loss, test_worst


if __name__ == '__main__':
    main()
